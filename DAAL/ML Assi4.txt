Assignment 4

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#Categeorized in ham and spam
#label and  text
df=pd.read_csv('SMSSpamCollection' ,sep='\t', names=['label','text'])

df

df.shape
#have to clean data
#delete punctuation delete was,any,for,them
#(stopwords-repeatively comes in english (regularly occured)) and delete ed(staining)
#text preprocessing 

!pip install nltk

import nltk

import nltk
nltk.download('stopwords')

sent = 'How are you friends?'

from nltk.tokenize import word_tokenize
word_tokenize(sent)

from nltk.corpus import stopwords
swords = stopwords.words('english')

clean = [word for word in word_tokenize(sent) if word not in swords]

clean

# Stemming words with NLTK
from nltk.stem import PorterStemmer
ps = PorterStemmer()
clean = [ps.stem(word) for word in word_tokenize(sent) 
         if word not in swords]
clean

sent = 'Hello friends! How are you? We will learning python today'

def clean_text(sent):
    tokens = word_tokenize(sent)
    clean = [word for word in tokens if word.isdigit() or word.isalpha()]
    clean = [ps.stem(word) for word in clean
         if word not in swords]
    return clean

clean_text(sent)

# Pre-processing 
#term frequnecy(inverse)
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(analyzer=clean_text)

x = df['text']
y = df['label']

x_new = tfidf.fit_transform(x)

x.shape

x_new.shape
#tfidf.get_feature_names()

y.value_counts()

import seaborn as sns
sns.countplot(x=y)

#cross validation
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x_new,y,test_size=0.25,random_state=1)

print(f"Size of splitted data")
print(f"x_train {x_train.shape}")
print(f"y_train {y_train.shape}")
print(f"y_test {x_test.shape}")
print(f"y_test {y_test.shape}")

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()
nb.fit(x_train.toarray(),y_train)
y_pred_nb = nb.predict(x_test.toarray())

y_test.value_counts()

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt

ConfusionMatrixDisplay.from_predictions(y_test,y_pred_nb)
plt.title('Naive bayes')
plt.show()
print(f" Accuracy is {accuracy_score(y_test,y_pred_nb)}")
print(classification_report(y_test,y_pred_nb))

from sklearn.ensemble import RandomForestClassifier
model_rf = RandomForestClassifier(random_state=1)
model_rf.fit(x_train,y_train)

y_pred_rf = model_rf.predict(x_test) #float

ConfusionMatrixDisplay.from_predictions(y_test,y_pred_rf)
plt.title('Random Forest')
plt.show()
print(f" Accuracy is {accuracy_score(y_test,y_pred_rf)}")
print(classification_report(y_test,y_pred_rf))

from sklearn.linear_model import LogisticRegression
model_lr = LogisticRegression(random_state=1)
model_lr.fit(x_train,y_train)
y_pred_lr = model_lr.predict(x_test)

ConfusionMatrixDisplay.from_predictions(y_test,y_pred_lr)
plt.title('Logistic regression')
plt.show()
print(f" Accuracy is {accuracy_score(y_test,y_pred_lr)}")
print(classification_report(y_test,y_pred_lr))

#hyper Parameter Tunning(changle Parameter)(cross_validation cha class)
from sklearn.model_selection import GridSearchCV

para = {
    
    'criterion':['gini', 'entropy','log_loss'],
#     'max_features': ['sqrt','log2'],
#     'random_state': [0,1,2,3,4],
    'class_weight':['balanced','balanced_subsample']
}

grid = GridSearchCV(model_rf, param_grid=para, cv=5, scoring='accuracy')

grid.fit(x_train,y_train)

rf = grid.best_estimator_

y_pred_grid = rf.predict(x_test)

ConfusionMatrixDisplay.from_predictions(y_test,y_pred_grid)
plt.title('Gride Search')
plt.show()
print(f" Accuracy is {accuracy_score(y_test,y_pred_grid)}")
print(classification_report(y_test,y_pred_grid))

Assignment 5

#unsupervised learning
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#2 features for visualization
#no need to do for clustering train test split(unsupervised no need)
df=pd.read_csv('Mall_Customers.csv')

df

#Annual Income (k$) & Spending Score (1-100) taking two columns
x=df.iloc[:,3:]

x

#which group values are similar
plt.title('Unclustered Data')
plt.xlabel('Annual Income')
plt.ylabel('Spending Score')
plt.scatter(x['Annual Income (k$)'], x['Spending Score (1-100)'])
# Show the plot
plt.show()

#elbow method(correct value of k,perf eval. sse inversely of k.using sum of square error #for some value of k)
#silhouette method(ideal number of clusters)
#elbow point means that point in which value in graph is start decreasing
from sklearn.cluster import KMeans, AgglomerativeClustering
#AgglomerativeClustering(heiarchical Clustering method)

km=KMeans(n_clusters=3)

x.shape

km.fit_predict(x)

#Sum Square error
km.inertia_

sse=[]
for k in range(1,16):
    km=KMeans(n_clusters=k)
    km.fit_predict(x)
    sse.append(km.inertia_)

sse

plt.title('Elbow Method')
plt.xlabel('Value of K')
plt.ylabel('SSE')
plt.grid()
plt.xticks(range(1,16))
plt.plot(range(1,16),sse,marker='.',color='red')
#ideal of value of k is 5 

#value of silhoutte score is -1(points far) t 1(close)
#optimal value of k
#more silhoutee score that is our ideal value of k
from sklearn.metrics import silhouette_score

silh=[]
for k in range(2,16):
    km=KMeans(n_clusters=k)
    labels=km.fit_predict(x)
    score=silhouette_score(x,labels)
    silh.append(score)

silh

plt.title('Silhouette Method')
plt.xlabel('Value of K')
plt.ylabel('Silhouette Score')
plt.grid()
plt.xticks(range(2,16))
plt.plot(range(2,16),silh,marker='.',color='red')

plt.title('Silhouette Method')
plt.xlabel('Value of K')
plt.ylabel('Silhouette Score')
plt.grid()
plt.xticks(range(2,16))
plt.bar(range(2,16),silh,color='red')
#optimal value of k is 5

#how to create cluster
km=KMeans(n_clusters=5,random_state=0)

labels=km.fit_predict(x)

labels

cent=km.cluster_centers_

plt.figure(figsize=(16,9))
plt.subplot(1,2,1)
plt.title('Unclustered Data')
plt.xlabel('Annual Income')
plt.ylabel('Spending Score')
plt.scatter(x['Annual Income (k$)'], x['Spending Score (1-100)'])
plt.subplot(1,2,2)
plt.title('Clustered Data')
plt.xlabel('Annual Income')
plt.ylabel('Spending Score')
plt.scatter(x['Annual Income (k$)'], x['Spending Score (1-100)'],c=labels)
plt.scatter(cent[:,0],cent[:,1],s=100,color='k')

km.cluster_centers_

km.inertia_

km.labels_

four=df[labels==4]

four.to_csv('mydata.csv')

km.predict([[26,61]])

#hierachical Clustering

agl=AgglomerativeClustering(n_clusters=5)

alabels=agl.fit_predict(x)

alabels

plt.figure(figsize=(16,9))
plt.subplot(1,2,1)
plt.title('Agglomerative')
plt.xlabel('Annual Income')
plt.ylabel('Spending Score')
plt.scatter(x['Annual Income (k$)'], x['Spending Score (1-100)'],
           c=alabels)
plt.subplot(1,2,2)
plt.title('KMeans')
plt.xlabel('Annual Income')
plt.ylabel('Spending Score')
plt.scatter(x['Annual Income (k$)'], x['Spending Score (1-100)'],c=labels)
plt.scatter(cent[:,0],cent[:,1],s=100,color='k')


import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

# Assuming 'x' is your data
# agl and alabels are from your AgglomerativeClustering code

# Perform hierarchical clustering and obtain the linkage matrix
linkage_matrix = linkage(x, method='ward')  # 'ward' is just one example of the linkage method

# Plot the dendrogram
plt.figure(figsize=(12, 8))
dendrogram(linkage_matrix, orientation='top', labels=alabels, distance_sort='descending', show_leaf_counts=True)
plt.title('Dendrogram for Agglomerative Clustering')
plt.xlabel('Data Points')
plt.ylabel('Distance')
plt.show()


Assignment 6

#unsupervised learning
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
#confidence(kiti % perfect ahe)
#support(freuqncy)

df=pd.read_csv('Market_Basket_Optimisation.csv')

df

df.shape

!pip install mlxtend

import csv
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules

data = []
with open('Market_Basket_Optimisation.csv') as file:
    reader = csv.reader(file, delimiter=',')
    for row in reader:
        data +=[row]

data[1:10] #list of 10 list

#structured dataset isrequired to performs operation
len(data)

#unique item cha column and ghetla tr true
te = TransactionEncoder()
x = te.fit_transform(data)

#true and false
x

te.columns_
len(te.columns_)

df.head()

df = pd.DataFrame(x, columns=te.columns_)

df

# Finding frequent itemsets support(10% yeyala hava tarech valid strong rule and weak #rule)
freq_itemset = apriori(df, min_support=0.1, use_colnames=True)

freq_itemset

 #Find the rules
rules = association_rules(freq_itemset, metric='confidence', min_threshold=0.01)

rules = rules[['antecedents','consequents','support','confidence']]
rules

rules[rules['antecedents'] == {'cake'}]['consequents']


import matplotlib.pyplot as plt

# Assuming 'rules' DataFrame contains your association rules
# Replace 'cake' with the actual item you are interested in

# Print the DataFrame for verification
print(rules)

# Extract data for visualization
antecedents = rules['antecedents'].apply(lambda x: list(x)[0])
consequents = rules['consequents'].apply(lambda x: list(x)[0])
support = rules['support']
confidence = rules['confidence']

# Scatter plot for support and confidence
plt.figure(figsize=(12, 8))
plt.scatter(support, confidence, s=100, c='blue', alpha=0.5, edgecolors="w", linewidth=1)

# Annotate each point with the corresponding antecedent and consequent
for i, txt in enumerate(zip(antecedents, consequents)):
    plt.annotate(f"{txt[0]} -> {txt[1]}", (support[i], confidence[i]), fontsize=8, ha='right')

# Set labels and title
plt.xlabel('Support')
plt.ylabel('Confidence')
plt.title('Association Rules Visualization')

plt.show()








